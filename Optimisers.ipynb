{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression"
      ],
      "metadata": {
        "id": "9VXi0avgKsnn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating the data\n",
        "X, y = make_regression(n_samples = 1000, n_features = 10)"
      ],
      "metadata": {
        "id": "Yx_ftJguKsp9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def config():\n",
        "\n",
        "  global w, b, n, learning_rate\n",
        "  w = np.random.rand(10)\n",
        "  b = np.random.rand(1)\n",
        "\n",
        "  n = y.shape[0]\n",
        "  learning_rate = 0.01\n"
      ],
      "metadata": {
        "id": "CuNoDyh9AxPD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Gradient Descent;\n",
        "\n",
        "def gradient_descent(w, b, X, y):\n",
        "\n",
        "\n",
        "  y_hat = X @ w + b\n",
        "\n",
        "  loss = 1 / n * np.sum((y - y_hat) ** 2)\n",
        "\n",
        "  dw = 2 / n *  np.sum(X.T @ ( y_hat - y ))\n",
        "  db = 2 / n *  np.sum( y_hat - y )\n",
        "\n",
        "\n",
        "  w = w - learning_rate * dw\n",
        "  b = b - learning_rate * db\n",
        "\n",
        "\n",
        "  return w, b, loss"
      ],
      "metadata": {
        "id": "O81uNoJkBKIJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "config()\n",
        "\n",
        "for i in range(epochs):\n",
        "  w, b, loss = gradient_descent(w, b, X, y)\n",
        "  print(f'Epoch {i} - Loss = ', loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A3cty_sAzgK",
        "outputId": "357b4ac2-4095-4f20-921f-d968343ce07c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss =  34404.70324872005\n",
            "Epoch 1 - Loss =  24887.1957086164\n",
            "Epoch 2 - Loss =  18702.30586156976\n",
            "Epoch 3 - Loss =  14683.06442813652\n",
            "Epoch 4 - Loss =  12071.136279537686\n",
            "Epoch 5 - Loss =  10373.73018804087\n",
            "Epoch 6 - Loss =  9270.614081931437\n",
            "Epoch 7 - Loss =  8553.690503366337\n",
            "Epoch 8 - Loss =  8087.730755689358\n",
            "Epoch 9 - Loss =  7784.858662240764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeGjdJ5iI7OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nPAM-yb3I7Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### RMS Prop;\n",
        "\n",
        "def RmsProp(X, y, w, b, learning_rate=0.01, beta=.9, iterations = 10, epsilon=1e-7):\n",
        "\n",
        "    v_tw = np.zeros_like(w)\n",
        "    v_tb = np.zeros_like(b)\n",
        "\n",
        "    # RMSprop optimization loop\n",
        "    for i in range(iterations):\n",
        "        # Compute the gradient of the loss function\n",
        "        y_hat =  X @ w + b\n",
        "        loss = 1 / n * np.sum((y - y_hat) ** 2)\n",
        "\n",
        "        print(f'Epoch {i} - Loss = ', loss)\n",
        "\n",
        "        dw = 2 / n *  np.sum(X.T @ ( y_hat - y ))\n",
        "        db = 2 / n *  np.sum( y_hat - y )\n",
        "\n",
        "        # Update the exponentially decaying average of squared gradients  ## learning rate correction\n",
        "        v_tw = beta * v_tw + (1 - beta) * dw**2\n",
        "        v_tb = beta * v_tb + (1 - beta) * db**2\n",
        "\n",
        "        # Update the parameters using RMSprop\n",
        "        w -= (learning_rate / (np.sqrt(v_tw) + epsilon)) * dw\n",
        "        b -= (learning_rate / (np.sqrt(v_tb) + epsilon)) * db\n",
        "\n",
        "    return w, b\n"
      ],
      "metadata": {
        "id": "zZVp9mX3AxRd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config()\n",
        "\n",
        "w, b = RmsProp(X, y, w, b, learning_rate, beta=.9, iterations = 10, epsilon=1e-7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4elmkfTcIvZb",
        "outputId": "63a47d20-a31d-4ca3-eb78-48a0017de4a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss =  34280.04033629526\n",
            "Epoch 1 - Loss =  34246.49248776484\n",
            "Epoch 2 - Loss =  34222.17607837403\n",
            "Epoch 3 - Loss =  34201.828366860886\n",
            "Epoch 4 - Loss =  34183.775088020215\n",
            "Epoch 5 - Loss =  34167.23873325005\n",
            "Epoch 6 - Loss =  34151.78588980398\n",
            "Epoch 7 - Loss =  34137.14690818973\n",
            "Epoch 8 - Loss =  34123.14118429671\n",
            "Epoch 9 - Loss =  34109.64116988388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DZZHIFr2LJTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Adam;\n",
        "\n",
        "def Adam(X, y, w, b, learning_rate=0.01, beta=.9, iterations = 10, epsilon=1e-7):\n",
        "\n",
        "    v_tw = np.zeros_like(w)\n",
        "    v_tb = np.zeros_like(b)\n",
        "\n",
        "    m_tw = np.zeros_like(w)\n",
        "    m_tb = np.zeros_like(b)\n",
        "\n",
        "    # RMSprop optimization loop\n",
        "    for i in range(iterations):\n",
        "        # Compute the gradient of the loss function\n",
        "        y_hat =  X @ w + b\n",
        "        loss = 1 / n * np.sum((y - y_hat) ** 2)\n",
        "\n",
        "        print(f'Epoch {i} - Loss = ', loss)\n",
        "\n",
        "        dw = 2 / n *  np.sum(X.T @ ( y_hat - y ))\n",
        "        db = 2 / n *  np.sum( y_hat - y )\n",
        "\n",
        "        # Update the exponentially decaying average of gradients  ## gradient correction\n",
        "        m_tw = beta * m_tw + (1 - beta) * dw\n",
        "        m_tb = beta * m_tb + (1 - beta) * db\n",
        "\n",
        "        # Update the exponentially decaying average of squared gradients  ## learning rate correction\n",
        "        v_tw = beta * v_tw + (1 - beta) * dw**2\n",
        "        v_tb = beta * v_tb + (1 - beta) * db**2\n",
        "\n",
        "        # Update the parameters using RMSprop\n",
        "        w -= (learning_rate / (np.sqrt(v_tw) + epsilon)) * m_tw\n",
        "        b -= (learning_rate / (np.sqrt(v_tb) + epsilon)) * m_tb\n",
        "\n",
        "    return w, b\n"
      ],
      "metadata": {
        "id": "IC1Al95BLJYd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config()\n",
        "\n",
        "w, b = Adam(X, y, w, b, learning_rate, beta=.9, iterations = 10, epsilon=1e-7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXukw-nsLJbE",
        "outputId": "ab277ac5-1f06-4a44-db67-38d645fe1a54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss =  33933.79141709444\n",
            "Epoch 1 - Loss =  33930.45492207886\n",
            "Epoch 2 - Loss =  33925.85624904302\n",
            "Epoch 3 - Loss =  33920.3646833473\n",
            "Epoch 4 - Loss =  33914.17915972361\n",
            "Epoch 5 - Loss =  33907.43021364352\n",
            "Epoch 6 - Loss =  33900.2120849146\n",
            "Epoch 7 - Loss =  33892.59674750191\n",
            "Epoch 8 - Loss =  33884.641228567794\n",
            "Epoch 9 - Loss =  33876.39188709095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWE5KYOeKmO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}